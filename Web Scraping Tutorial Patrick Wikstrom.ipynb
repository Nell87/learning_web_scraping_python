{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Extract album title from a single item on a single page\n",
    "SOURCE: https://github.com/qut-dmrc/web-scraping-intro-workshop/blob/master/web-scraping-intro-step1.ipynb\n",
    "\n",
    "This notebook gets a page from the Metacritic website and then extracts one of the fields we are interested in from it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import python modules\n",
    "import bs4       # BeautifulSoup4 is a Python package for parsing HTML and XML documents\n",
    "import requests  # It allows you to send HTTP requests in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The next steps build up the URL that has the information we want. The sections of the url that we will want to change to get more pages of information are kept seperate so we can change them more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is the base_url\n",
    "base_url = \"http://www.metacritic.com/browse/albums/artist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Select which page to scrape based on the first letter of the artist names\n",
    "letter = \"/a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build the url (only scrape the first page - page 0)\n",
    "page = base_url+letter+\"?page=0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now lets check what the variable thepage is set to. You can show the value of any variable in a notebook by putting it in the last line of a notebook cell and running the cell. Jupyter will try to display it in a clear way, often clearer than the default 'print' layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.metacritic.com/browse/albums/artist/a?page=0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These steps get the page using Requests and then process it using BeautifulSoup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# the bot pretends to be a Chrome browser\n",
    "hdrs = {\"User-Agent\": \"Chrome/78.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# call the url\n",
    "response_url = requests.get(page, headers=hdrs)   # We can see the status code that the server returned. \n",
    "                                                  # If the server returns 200 status code, the program will work!\n",
    "                                                  # If the server returns 404, the program will fail.\n",
    "if response_url.status_code == 200:\n",
    "    print(\"Success!\")\n",
    "    \n",
    "elif response_url.status_code == 404:\n",
    "    print(\"Not found!\")                           # It works!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Transform to soup using html.parser \n",
    "soup = bs4.BeautifulSoup(response_url.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"product_wrap\">\n",
       "<div class=\"basic_stat product_title\">\n",
       "<a href=\"/music/colonia/a-camp\">\n",
       "                            Colonia\n",
       "                                                    </a>\n",
       "</div>\n",
       "<div class=\"basic_stat product_score brief_metascore\">\n",
       "<div class=\"metascore_w small release positive\">64</div>\n",
       "</div>\n",
       "<div class=\"basic_stat condensed_stats\">\n",
       "<ul class=\"more_stats\">\n",
       "<li class=\"stat product_artist\">\n",
       "<span class=\"label\">Artist:</span>\n",
       "<span class=\"data\">A Camp</span>\n",
       "</li>\n",
       "<li class=\"stat product_avguserscore\">\n",
       "<span class=\"label\">User:</span>\n",
       "<span class=\"data textscore textscore_favorable\">8.0</span>\n",
       "</li>\n",
       "<li class=\"stat release_date full_release_date\">\n",
       "<span class=\"label\">Release Date:</span>\n",
       "<span class=\"data\">Apr 28, 2009</span>\n",
       "</li>\n",
       "</ul>\n",
       "</div>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all div-tags of class \"product_wrap\" (We found it using the SelectorGadget extension)\n",
    "title_tag = soup.find_all(\"div\", class_=[\"product_wrap\"]) \n",
    "\n",
    "# Have a look at the first item in the list\n",
    "title_tag[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"basic_stat product_title\">\n",
       "<a href=\"/music/colonia/a-camp\">\n",
       "                            Colonia\n",
       "                                                    </a>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the first div-tag from the first item\n",
    "thetitle = title_tag[0].find(\"div\", class_=\"product_title\")\n",
    "thetitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n                            Colonia\\n                                                    \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The album name is the text part of this tag\n",
    "temp= thetitle.get_text()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Colonia']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's poorly formatted so we need to clean it up a bit by first splitting the string into a list of words\n",
    "temptemp= temp.split()\n",
    "temptemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Colonia'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And then we need to join the words back together with single spaces between them\n",
    "clean_title = \" \".join(temptemp)\n",
    "clean_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all album titles on a single page\n",
    "SOURCE: https://github.com/qut-dmrc/web-scraping-intro-workshop/blob/master/web-scraping-intro-step2.ipynb\n",
    "\n",
    "This notebook extends the previous step to get all of the titles from a single page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We already have bs4 and requests python modules, so we don't need to import more modules. \n",
    "- We also have the base_url, the lett, the page and the browser\n",
    "- We have checked that the status code is 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.metacritic.com/browse/albums/artist/a?page=0'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to soup using html.parser parser\n",
    "soup = bs4.BeautifulSoup(response_url.text, \"html.parser\")\n",
    "\n",
    "# Find all div-tags of class \"product_wrap\" (We found it using the SelectorGadget extension)\n",
    "title_tag = soup.find_all(\"div\", class_=[\"product_wrap\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now process **title_tag** in a new way to get all the items instead of just one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the same thing as in the previous step but for all items in the page\n",
    "\n",
    "list = []\n",
    "\n",
    "for item in title_tag:\n",
    "    \n",
    "    # extract the first div-tag from the item\n",
    "    thetitle = item.find(\"div\", class_=\"product_title\")\n",
    "    \n",
    "    # extract and clean up the album name\n",
    "    temptemp = thetitle.get_text()\n",
    "    temptemp= temptemp.split()\n",
    "    album_name = \" \".join(temptemp)\n",
    "    \n",
    "    # add the albun name to the list\n",
    "    list += [album_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Colonia',\n",
       " 'Call It Blazing',\n",
       " 'Common Courtesy',\n",
       " 'Bad Vibrations',\n",
       " 'Pines',\n",
       " 'Pile',\n",
       " 'Toy',\n",
       " 'Feathers Wet, Under the Moon',\n",
       " 'Wooden Mask',\n",
       " 'Passover',\n",
       " \"You're Always on My Mind\",\n",
       " 'A Gun Called Tension',\n",
       " 'Essence',\n",
       " 'Darkness At Noon',\n",
       " 'The Way The Wind Blows',\n",
       " 'Cervantine',\n",
       " 'You Have Already Gone to the Other World',\n",
       " 'And Hell Will Follow Me',\n",
       " 'Thirteenth Step',\n",
       " 'eMOTIVe',\n",
       " 'Eat the Elephant',\n",
       " 'A Place To Bury Strangers',\n",
       " 'Exploding Head',\n",
       " 'Onwards to the Wall [EP]',\n",
       " 'Worship',\n",
       " 'Transfixiation',\n",
       " 'Pinned',\n",
       " 'Elasticity',\n",
       " 'Ashes Grammar',\n",
       " 'Nitetime Rainbows [EP]',\n",
       " 'Autumn, Again',\n",
       " 'Sea When Absent',\n",
       " 'We Got It From Here...Thank You 4 Your Service',\n",
       " 'Partycrasher',\n",
       " 'A Winged Victory for the Sullen',\n",
       " 'Atomos',\n",
       " 'Iris [Original Motion Picture Soundtrack]',\n",
       " 'The Undivided Five',\n",
       " 'Trap Lord',\n",
       " 'Always Strive and Prosper',\n",
       " 'Still Striving [Mixtape]',\n",
       " 'Cozy Tapes, Vol. 1: Friends',\n",
       " 'Cozy Tapes, Vol. 2: Too Cozy',\n",
       " 'Live Love A$AP',\n",
       " 'Long.Live.A$AP',\n",
       " 'At.Long.Last.A$AP',\n",
       " 'Testing',\n",
       " 'Teen Spirit',\n",
       " 'Foot Of The Mountain',\n",
       " \"When The Devil's Loose\",\n",
       " 'Believers',\n",
       " 'Enderness',\n",
       " 'You Are the One',\n",
       " '2012-2017',\n",
       " 'The Slow Wonder',\n",
       " 'Get Guilty',\n",
       " 'Shut Down The Streets',\n",
       " 'A.R.E. Weapons',\n",
       " 'Aaliyah',\n",
       " 'I Heart California',\n",
       " 'Modern Jester',\n",
       " 'Marvelous Clouds',\n",
       " 'Memphis Rain',\n",
       " 'Silver Tears',\n",
       " 'Karma for Cheap',\n",
       " 'My True Story',\n",
       " 'Apache',\n",
       " 'Invisible Cinema',\n",
       " \"We Don't Have Each Other\",\n",
       " 'Control System',\n",
       " 'These Days...',\n",
       " 'Do What Thou Wilt.',\n",
       " 'The Lexicon of Love II',\n",
       " 'Fangnawa Experience',\n",
       " 'Skeleton',\n",
       " 'Reviver [EP]',\n",
       " 'Crush',\n",
       " 'City of Refuge',\n",
       " 'Start and Complete',\n",
       " 'Between the Walls',\n",
       " 'Victory Shorts',\n",
       " 'Absolutely Free',\n",
       " 'Black Ice',\n",
       " 'Backtracks',\n",
       " 'Rock or Bust',\n",
       " 'The Rise of Chaos',\n",
       " 'Colliding by Design',\n",
       " 'Space Invader',\n",
       " 'Trials & Tribulations',\n",
       " 'Ace of Cups',\n",
       " 'Deathless Master',\n",
       " 'York Blvd',\n",
       " '1992-2001',\n",
       " 'Love & Hate',\n",
       " 'Magnificent City',\n",
       " 'Musique de France',\n",
       " 'Music Sounds Better With You',\n",
       " 'MST',\n",
       " 'Living With A Tiger']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all review data from a single page\n",
    "SOURCE: https://github.com/qut-dmrc/web-scraping-intro-workshop/blob/eed5d2a9dcc328dc2988b31ac0b8adadc2f0561c/web-scraping-intro-step3.ipynb\n",
    "\n",
    "Extend from getting just the title field to getting all the fields we are interested in from each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "178.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
