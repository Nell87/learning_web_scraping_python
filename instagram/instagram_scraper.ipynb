{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOURCES: \n",
    "When the Entire Page Has Infinite Scroll\n",
    "- https://michaeljsanders.com/2017/05/12/scrapin-and-scrollin.html\n",
    "\n",
    "General Web Scraping Tutorials\n",
    "- https://github.com/pwikstrom/build-a-bot\n",
    "- https://medium.com/@srujana.rao2/scraping-instagram-with-python-using-selenium-and-beautiful-soup-8b72c186a058\n",
    "\n",
    "Remove duplicates from list\n",
    "- https://thispointer.com/python-how-to-remove-duplicates-from-a-list/\n",
    "\n",
    "Get the JSON from the HTML\n",
    "- https://python-forum.io/Thread-ReGex-With-Python?page=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic tips about jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOURCE: https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/\n",
    "\n",
    "- **Esc** will take you into command mode where you can navigate around your notebook with arrow keys.\n",
    "\n",
    "- In command mode:\n",
    "    - **A** to insert a new cell above the current cell, **B** to insert a new cell below.\n",
    "    - **M** to change the current cell to Markdown, **Y** to change it back to code\n",
    "    - **D + D** (press the key twice) to delete the current cell\n",
    "    \n",
    "- **Enter** will take you from command mode back into edit mode for the given cell.\n",
    "- You can also use **Shift + M** to merge multiple cells.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python modules\n",
    "import bs4        # BeautifulSoup4 is a Python package for parsing HTML and XML documents\n",
    "import time       # We need to wait 3 seconds every time that we scroll in instagram\n",
    "import requests   # It allows you to send HTTP requests in Python\n",
    "import re\n",
    "import json \n",
    "\n",
    "from selenium import webdriver   # A collection of language specific bindings to drive a browser \n",
    "from webdriver_manager.chrome import ChromeDriverManager   # allows to automate the management of the binary drivers \n",
    "                                                           # (e.g. chromedriver, geckodriver...) required by Selenium WebDriver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashtag/s\n",
    "hashtag='laveganesa'\n",
    "\n",
    "# The bot pretends to be a Chrome browser\n",
    "hdrs = {\"User-Agent\": \"Chrome/78.0\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicates from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove duplicates (keepping the order of unique elements as it was in the original list)\n",
    "def removeDuplicates(list_elements):\n",
    "    \n",
    "    # Create an empty list to store unique elements\n",
    "    unique_list = []\n",
    "    \n",
    "    # Iterate over the original list and for each element\n",
    "    # add it to uniqueList, if its not already there.\n",
    "    for elem in list_elements:\n",
    "        if elem not in unique_list:\n",
    "            unique_list.append(elem)\n",
    "    \n",
    "    # Return the list of unique elements        \n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scroll and store href of every post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium script to scroll to the bottom. We need to wait 3 seconds to the next batch of data to load, then continue scrolling\n",
    "# It will continue to do this until the page stops loading new data.\n",
    "# Meanwhile, we'll store in a list all the href from every post\n",
    "\n",
    "def get_href_instagram(hashtag):\n",
    "    \n",
    "    # Initialise browser\n",
    "    browser = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    browser.get('https://www.instagram.com/explore/tags/'+hashtag)   \n",
    "\n",
    "    # Scrolling and storing process    \n",
    "    length_page = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "\n",
    "    match=False\n",
    "    list_href=[]\n",
    "\n",
    "    while(match==False):\n",
    "        last_count = length_page\n",
    "        time.sleep(3)\n",
    "        length_page = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "\n",
    "        if last_count==length_page:\n",
    "            match=True\n",
    "\n",
    "        # Grab the source code\n",
    "        source = browser.page_source\n",
    "\n",
    "        # Transform to soup using html.parser (beautify)\n",
    "        soup = bs4.BeautifulSoup(source, \"html.parser\")\n",
    "\n",
    "        # Find all div-tags of class \"v1Nh3 kIKUG  _bz0w\" \n",
    "        links_posts = soup.find_all(\"div\", class_=[\"v1Nh3 kIKUG _bz0w\"]) \n",
    "\n",
    "        # Extract the href from every post\n",
    "        for post in links_posts:\n",
    "\n",
    "            ind_link = post.find(\"a\")\n",
    "            href = \"https://www.instagram.com\" + ind_link.get(\"href\")\n",
    "            list_href +=[href]\n",
    "            \n",
    "    # Remove the duplicates\n",
    "    final_list_href = removeDuplicates(list_href)  \n",
    "    \n",
    "    return final_list_href"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for [chromedriver 78.0.3904.70 win32] driver in cache \n",
      "File found in cache by path [C:\\Users\\saram\\.wdm\\drivers\\chromedriver\\78.0.3904.70\\win32\\chromedriver.exe]\n"
     ]
    }
   ],
   "source": [
    "# Get the href of every post\n",
    "href_everylink = get_href_instagram(hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.instagram.com/p/B3xXXFehcoV/'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get information from every post\n",
    "first_post = href_everylink[0]\n",
    "first_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Call the url\n",
    "response_url = requests.get(first_post, headers=hdrs) \n",
    "\n",
    "# 2. Get the JSON from the url. There we'll find the graphql with the data we want\n",
    "source = response_url.text\n",
    "data_json = re.findall(r'<script type=\"text/javascript\">window._sharedData = (.*);</script>', sorurce)[0]\n",
    "data_json = json.loads(data_json)     # it gives back a python dictionary.\n",
    "\n",
    "# 3. Let's go to the section we're interested in\n",
    "data = data_json['entry_data']['PostPage'][0]['graphql']['shortcode_media']   # Here ['PostPage'][0] contain a list (it has [])\n",
    "                                                                              # therefore, we use [0] to get get contented  \n",
    "                                                                              # inside this list and continue navigating.                                                                        \n",
    "# 4. Let's get interesting information!!\n",
    "likes = data['edge_media_preview_like']['count']\n",
    "likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Extract graphql from the __additionalDataLoaded function"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
